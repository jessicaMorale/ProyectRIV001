# Importa las librer√≠as necesarias
import ir_datasets
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import string
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import math
from collections import Counter, defaultdict

# Descargar recursos necesarios de NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Funci√≥n que imprime una l√≠nea separadora
def print_separator():
    print("=" * 60)

# Funci√≥n de preprocesamiento de texto
def preprocess_text(text):
    """
    Procesamiento b√°sico del texto:
    - Tokenizaci√≥n
    - Normalizaci√≥n (min√∫sculas, remoci√≥n de puntuaci√≥n)
    - Remoci√≥n de stopwords
    """
    if not text:
        return ""
    
    # NORMALIZACI√ìN
    text = text.lower()
    text = re.sub(r'[^a-z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # TOKENIZACI√ìN
    tokens = word_tokenize(text)
    
    # REMOCI√ìN DE STOPWORDS
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [token for token in tokens if token not in stop_words]
    
    return ' '.join(filtered_tokens)

# Funci√≥n para construir √≠ndice invertido
def build_inverted_index(processed_docs):
    """
    Construye un √≠ndice invertido que almacena:
    - Para cada t√©rmino: los documentos donde aparece y su frecuencia
    """
    print("üîÑ Construyendo √≠ndice invertido...")
    inverted_index = {}
    
    for doc_id, doc in enumerate(processed_docs):
        if not doc.strip():
            continue
            
        # Tokenizar el documento procesado
        tokens = doc.split()
        
        # Contar frecuencia de cada t√©rmino en este documento
        term_freq = {}
        for token in tokens:
            term_freq[token] = term_freq.get(token, 0) + 1
        
        # Agregar al √≠ndice invertido
        for term, freq in term_freq.items():
            if term not in inverted_index:
                inverted_index[term] = {}
            inverted_index[term][doc_id] = freq
    
    print(f"‚úÖ √çndice invertido construido con {len(inverted_index)} t√©rminos √∫nicos")
    return inverted_index

# Clase BM25 para recuperaci√≥n
class BM25:
    def __init__(self, documents, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.documents = documents
        self.doc_lengths = [len(doc.split()) for doc in documents]
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths)
        self.doc_count = len(documents)
        
        # Construir vocabulario y frecuencias de documentos
        self.vocab = set()
        self.doc_frequencies = {}
        
        for doc in documents:
            words = set(doc.split())
            self.vocab.update(words)
            for word in words:
                self.doc_frequencies[word] = self.doc_frequencies.get(word, 0) + 1
    
    def get_scores(self, query):
        scores = []
        query_words = query.split()
        
        for doc_idx, doc in enumerate(self.documents):
            doc_words = doc.split()
            doc_word_counts = Counter(doc_words)
            score = 0
            
            for word in query_words:
                if word in doc_word_counts:
                    # Frecuencia del t√©rmino en el documento
                    tf = doc_word_counts[word]
                    
                    # Frecuencia inversa del documento
                    df = self.doc_frequencies.get(word, 0)
                    idf = math.log((self.doc_count - df + 0.5) / (df + 0.5))
                    
                    # Longitud del documento
                    doc_len = self.doc_lengths[doc_idx]
                    
                    # F√≥rmula BM25
                    numerator = tf * (self.k1 + 1)
                    denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avg_doc_length))
                    score += idf * (numerator / denominator)
            
            scores.append(score)
        
        return scores

# ================= FUNCIONES DE EVALUACI√ìN ===================

def calculate_precision_recall(retrieved_docs, relevant_docs):
    """
    Calcula Precision y Recall para una consulta.
    
    Args:
        retrieved_docs: Lista de doc_ids recuperados
        relevant_docs: Lista de doc_ids relevantes
    """
    # Convertir a conjuntos para intersecci√≥n
    retrieved_set = set(retrieved_docs)
    relevant_set = set(relevant_docs)
    
    # Calcular intersecci√≥n (documentos relevantes y recuperados)
    relevant_retrieved = retrieved_set.intersection(relevant_set)
    
    # Precision = |relevant ‚à© retrieved| / |retrieved|
    precision = len(relevant_retrieved) / len(retrieved_set) if retrieved_set else 0.0
    
    # Recall = |relevant ‚à© retrieved| / |relevant|
    recall = len(relevant_retrieved) / len(relevant_set) if relevant_set else 0.0
    
    return precision, recall

def show_index_stats(inverted_index, sample_terms=5):
    print(f"\nüìä ESTAD√çSTICAS DEL √çNDICE INVERTIDO:")
    print(f"   Total de t√©rminos: {len(inverted_index)}")
    
    # T√©rminos m√°s frecuentes (en m√°s documentos)
    term_doc_counts = [(term, len(docs)) for term, docs in inverted_index.items()]
    term_doc_counts.sort(key=lambda x: x[1], reverse=True)
    
    print(f"   T√©rminos en m√°s documentos:")
    for term, doc_count in term_doc_counts[:sample_terms]:
        total_freq = sum(inverted_index[term].values())
        print(f"     '{term}': {doc_count} documentos, {total_freq} ocurrencias totales")
    
    print(f"   T√©rminos en menos documentos:")
    for term, doc_count in term_doc_counts[-sample_terms:]:
        total_freq = sum(inverted_index[term].values())
        print(f"     '{term}': {doc_count} documentos, {total_freq} ocurrencias totales")

# Funci√≥n para mostrar el men√∫ principal
def show_main_menu():
    print("\n" + "="*60)
    print("üîç SISTEMA DE B√öSQUEDA DE DOCUMENTOS")
    print("="*60)
    print("1. üìã Ver estad√≠sticas del dataset")
    print("2. üìä Ver estad√≠sticas del √≠ndice invertido")
    print("3. üîç B√∫squeda con TF-IDF + Coseno")
    print("4. üéØ B√∫squeda con BM25")
    print("5. üìà Evaluaci√≥n de resultados")
    print("6. ‚ùå Salir")
    print("="*60)

# Funci√≥n para mostrar estad√≠sticas del dataset
def show_dataset_stats(docs, queries, qrels_dict, vectorizer, tfidf_matrix):
    print(f"\nüìä ESTAD√çSTICAS DEL DATASET:")
    print_separator()
    print(f"   Total de documentos: {len(docs)}")
    print(f"   Total de queries: {len(queries)}")
    print(f"   Queries con qrels: {len(qrels_dict)}")
    print(f"   Vocabulario TF-IDF: {len(vectorizer.vocabulary_)} t√©rminos")
    print(f"   Matriz TF-IDF: {tfidf_matrix.shape}")
    input("\nüì• Presiona Enter para continuar...")

# Interfaz de b√∫squeda con TF-IDF
def search_interface_tfidf(vectorizer, tfidf_matrix, docs, doc_ids):
    while True:
        print("\nüîç B√öSQUEDA CON TF-IDF + COSENO")
        print_separator()
        print("Escribe tu consulta (o 'atras' para volver al men√∫ principal)")
        query = input("\n> ").strip()
        

        if query.lower() == 'atras':
            break
        if not query:
            print("‚ùå Por favor ingresa una consulta v√°lida.")
            continue

        print(f"\nüîÑ Procesando consulta: '{query}'...")
        
        # Preprocesar la consulta igual que los documentos
        processed_query = preprocess_text(query)
        if not processed_query.strip():
            print("‚ùå La consulta no contiene t√©rminos v√°lidos despu√©s del preprocesamiento.")
            continue
            
        query_vector = vectorizer.transform([processed_query])
        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
        relevant_docs = [(i, sim) for i, sim in enumerate(similarities) if sim > 0]
        relevant_docs.sort(key=lambda x: x[1], reverse=True)

        if not relevant_docs:
            print("‚ùå No se encontraron documentos relevantes.")
            continue

        print(f"\nüìã RESULTADOS TF-IDF PARA: '{query}'")
        print(f"Se encontraron {len(relevant_docs)} documentos relevantes")
        print_separator()

        for rank, (doc_id, sim) in enumerate(relevant_docs[:5], start=1):
            print(f"\nüî∏ RESULTADO #{rank}")
            print(f"   Similitud Coseno: {sim:.4f}")
            print(f"   ID Documento: {doc_ids[doc_id]}")
            print(f"   Contenido: {docs[doc_id][:200].replace('\n', ' ')}...")
            if rank < 5 and rank < len(relevant_docs):
                print("   " + "‚îÄ" * 50)

        if len(relevant_docs) > 5:
            print(f"\n   üìÑ ... y {len(relevant_docs) - 5} documentos m√°s")

        input("\nüì• Presiona Enter para continuar...")

# Interfaz de b√∫squeda con BM25
def search_interface_bm25(bm25_model, docs, doc_ids):
    while True:
        print("\nüéØ B√öSQUEDA CON BM25")
        print_separator()
        print("Escribe tu consulta (o 'atras' para volver al men√∫ principal)")
        query = input("\n> ").strip()
        

        if query.lower() == 'atras':
            break
        if not query:
            print("‚ùå Por favor ingresa una consulta v√°lida.")
            continue

        print(f"\nüîÑ Procesando consulta: '{query}'...")
        
        # Preprocesar la consulta igual que los documentos
        processed_query = preprocess_text(query)
        if not processed_query.strip():
            print("‚ùå La consulta no contiene t√©rminos v√°lidos despu√©s del preprocesamiento.")
            continue
            
        scores = bm25_model.get_scores(processed_query)
        relevant_docs = [(i, score) for i, score in enumerate(scores) if score > 0]
        relevant_docs.sort(key=lambda x: x[1], reverse=True)

        if not relevant_docs:
            print("‚ùå No se encontraron documentos relevantes.")
            continue

        print(f"\nüìã RESULTADOS BM25 PARA: '{query}'")
        print(f"Se encontraron {len(relevant_docs)} documentos relevantes")
        print_separator()

        for rank, (doc_id, score) in enumerate(relevant_docs[:5], start=1):
            print(f"\nüî∏ RESULTADO #{rank}")
            print(f"   Puntuaci√≥n BM25: {score:.4f}")
            print(f"   ID Documento: {doc_ids[doc_id]}")
            print(f"   Contenido: {docs[doc_id][:200].replace('\n', ' ')}...")
            if rank < 5 and rank < len(relevant_docs):
                print("   " + "‚îÄ" * 50)

        if len(relevant_docs) > 5:
            print(f"\n   üìÑ ... y {len(relevant_docs) - 5} documentos m√°s")

        input("\nüì• Presiona Enter para continuar...")

# Funci√≥n de evaluaci√≥n de resultados
def evaluation_interface(vectorizer, tfidf_matrix, bm25_model, queries, qrels_dict, doc_id_to_index):
    print("\nüìà EVALUACI√ìN DE RESULTADOS")
    print_separator()
    print("Selecciona el m√©todo a evaluar:")
    print("1. TF-IDF + Coseno")
    print("2. BM25")
    print("3. Volver al men√∫ principal")
    
    option = input("\nSelecciona una opci√≥n (1-3): ").strip()
    
    if option == "3":
        return
    elif option not in ["1", "2"]:
        print("‚ùå Opci√≥n no v√°lida.")
        return
    
    # Seleccionar queries para evaluar
    print(f"\nüìã Total de queries disponibles: {len(queries)}")
    print(f"üìã Queries con qrels: {len(qrels_dict)}")
    
    try:
        num_queries = int(input("¬øCu√°ntas queries evaluar? (m√°ximo 50): "))
        num_queries = min(num_queries, 50, len(queries))
    except ValueError:
        num_queries = 10
        print(f"Usando valor por defecto: {num_queries} queries")
    
    print(f"\nüîÑ Evaluando {num_queries} queries...")
    
    total_precision = 0
    total_recall = 0
    evaluated_queries = 0
    
    for i, query in enumerate(queries[:num_queries]):
        query_id = query.query_id
        query_text = query.text
        
        # Verificar si hay qrels para esta query
        if query_id not in qrels_dict:
            continue
        
        relevant_doc_ids = qrels_dict[query_id]
        
        # Convertir doc_ids a √≠ndices
        relevant_indices = []
        for doc_id in relevant_doc_ids:
            if doc_id in doc_id_to_index:
                relevant_indices.append(doc_id_to_index[doc_id])
        
        if not relevant_indices:
            continue
        
        # Ejecutar b√∫squeda seg√∫n el m√©todo seleccionado
        processed_query = preprocess_text(query_text)
        if not processed_query.strip():
            continue
        
        if option == "1":  # TF-IDF
            query_vector = vectorizer.transform([processed_query])
            similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
            results = [(i, sim) for i, sim in enumerate(similarities) if sim > 0]
            results.sort(key=lambda x: x[1], reverse=True)
        else:  # BM25
            scores = bm25_model.get_scores(processed_query)
            results = [(i, score) for i, score in enumerate(scores) if score > 0]
            results.sort(key=lambda x: x[1], reverse=True)
        
        if not results:
            continue
        
        # Extraer √≠ndices de documentos recuperados (top 10)
        retrieved_indices = [doc_idx for doc_idx, score in results[:10]]
        
        # Calcular precision y recall
        precision, recall = calculate_precision_recall(retrieved_indices, relevant_indices)
        
        total_precision += precision
        total_recall += recall
        evaluated_queries += 1
        
        # Mostrar progreso cada 10 queries
        if evaluated_queries % 10 == 0:
            print(f"   Procesadas {evaluated_queries} queries...")
    
    # Mostrar resultados
    if evaluated_queries > 0:
        avg_precision = total_precision / evaluated_queries
        avg_recall = total_recall / evaluated_queries
        
        method_name = "TF-IDF + Coseno" if option == "1" else "BM25"
        
        print(f"\nüìä RESULTADOS DE EVALUACI√ìN - {method_name}")
        print_separator()
        print(f"Queries evaluadas: {evaluated_queries}")
        print(f"Precision promedio: {avg_precision:.4f}")
        print(f"Recall promedio: {avg_recall:.4f}")
    else:
        print("\n‚ùå No se pudieron evaluar queries con los datos disponibles.")
    
    input("\nüì• Presiona Enter para continuar...")

# Funci√≥n principal del men√∫
def main_menu(docs, doc_ids, vectorizer, tfidf_matrix, inverted_index, bm25_model, queries, qrels_dict, doc_id_to_index):
    while True:
        show_main_menu()
        option = input("\nSelecciona una opci√≥n (1-6): ").strip()
        
        if option == "1":
            show_dataset_stats(docs, queries, qrels_dict, vectorizer, tfidf_matrix)
        elif option == "2":
            show_index_stats(inverted_index)
            input("\nüì• Presiona Enter para continuar...")
        elif option == "3":
            search_interface_tfidf(vectorizer, tfidf_matrix, docs, doc_ids)
        elif option == "4":
            search_interface_bm25(bm25_model, docs, doc_ids)
        elif option == "5":
            evaluation_interface(vectorizer, tfidf_matrix, bm25_model, queries, qrels_dict, doc_id_to_index)
        elif option == "6":
            print("\nüëã ¬°Hasta luego!")
            break
        else:
            print("\n‚ùå Opci√≥n no v√°lida. Por favor selecciona 1-6.")
            input("üì• Presiona Enter para continuar...")

# ================= EJECUCI√ìN ===================

# Carga del dataset BEIR CQADupStack programmers
print("üîÑ Cargando dataset BEIR CQADupStack programmers...")
dataset = ir_datasets.load("beir/cqadupstack/programmers")

# Extraer documentos del dataset
docs = []
doc_ids = []
doc_id_to_index = {}

print("üîÑ Extrayendo documentos...")
for idx, doc in enumerate(dataset.docs_iter()):
    combined_text = ""
    if hasattr(doc, 'title') and doc.title:
        combined_text += doc.title + " "
    if hasattr(doc, 'text') and doc.text:
        combined_text += doc.text
    
    docs.append(combined_text.strip())
    doc_ids.append(doc.doc_id)
    doc_id_to_index[doc.doc_id] = idx

print(f"‚úÖ Dataset cargado: {len(docs)} documentos.")

# Extraer queries del dataset
print("üîÑ Extrayendo queries...")
queries = list(dataset.queries_iter())
print(f"‚úÖ Queries cargadas: {len(queries)} queries.")

# Extraer qrels (relevance judgments)
print("üîÑ Extrayendo qrels...")
qrels_dict = defaultdict(list)
for qrel in dataset.qrels_iter():
    if qrel.relevance > 0:  # Solo documentos relevantes
        qrels_dict[qrel.query_id].append(qrel.doc_id)

print(f"‚úÖ Qrels cargados: {len(qrels_dict)} queries con documentos relevantes.")

# Aplicar preprocesamiento a los documentos
print("üîÑ Aplicando preprocesamiento a los documentos...")
processed_docs = [preprocess_text(doc) for doc in docs]

# Construir √≠ndice invertido
inverted_index = build_inverted_index(processed_docs)

# Vectorizaci√≥n con preprocesamiento
print("üîÑ Vectorizando documentos...")
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(processed_docs)
print("‚úÖ Vectorizaci√≥n completada.")

# Construir modelo BM25
print("üîÑ Construyendo modelo BM25...")
bm25_model = BM25(processed_docs)
print("‚úÖ Modelo BM25 construido.")

print("\nüéâ Sistema listo!")

# Ejecutar el men√∫ principal
main_menu(docs, doc_ids, vectorizer, tfidf_matrix, inverted_index, bm25_model, queries, qrels_dict, doc_id_to_index)
