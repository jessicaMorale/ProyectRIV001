# Importa las librerÃ­as necesarias
import ir_datasets
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import string
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Descargar recursos necesarios de NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# FunciÃ³n que imprime una lÃ­nea separadora
def print_separator():
    print("=" * 60)

# FunciÃ³n de preprocesamiento de texto
def preprocess_text(text):
    """
    Procesamiento bÃ¡sico del texto:
    - TokenizaciÃ³n
    - NormalizaciÃ³n (minÃºsculas, remociÃ³n de puntuaciÃ³n)
    - RemociÃ³n de stopwords
    """
    if not text:
        return ""
    
    # NORMALIZACIÃ“N
    text = text.lower()
    text = re.sub(r'[^a-z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # TOKENIZACIÃ“N
    tokens = word_tokenize(text)
    
    # REMOCIÃ“N DE STOPWORDS
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [token for token in tokens if token not in stop_words]
    
    return ' '.join(filtered_tokens)

# Interfaz de bÃºsqueda
def search_interface_no_preprocess(vectorizer, tfidf_matrix, docs, doc_ids):
    while True:
        print("\nğŸ” BÃšSQUEDA DE DOCUMENTOS ")
        print_separator()
        print("Escribe tu consulta (o 'menu' para volver al menÃº principal)")
        query = input("\n> ").strip()
        

        if query.lower() == 'menu':
            break
        if not query:
            print("âŒ Por favor ingresa una consulta vÃ¡lida.")
            continue

        print(f"\nğŸ”„ Procesando consulta: '{query}'...")
        query_vector = vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
        relevant_docs = [(i, sim) for i, sim in enumerate(similarities) if sim > 0]
        relevant_docs.sort(key=lambda x: x[1], reverse=True)

        if not relevant_docs:
            print("âŒ No se encontraron documentos relevantes.")
            continue

        print(f"\nğŸ“‹ RESULTADOS PARA: '{query}'")
        print(f"Se encontraron {len(relevant_docs)} documentos relevantes")
        print_separator()

        for rank, (doc_id, sim) in enumerate(relevant_docs[:5], start=1):
            print(f"\nğŸ”¸ RESULTADO #{rank}")
            print(f"   Similitud: {sim:.4f}")
            print(f"   ID Documento: {doc_ids[doc_id]}")
            print(f"   Contenido: {docs[doc_id][:200].replace('\n', ' ')}...")
            if rank < 5 and rank < len(relevant_docs):
                print("   " + "â”€" * 50)

        if len(relevant_docs) > 5:
            print(f"\n   ğŸ“„ ... y {len(relevant_docs) - 5} documentos mÃ¡s")

        input("\nğŸ“¥ Presiona Enter para continuar...")

# ================= EJECUCIÃ“N ===================

# Carga del dataset BEIR CQADupStack programmers
print("ğŸ”„ Cargando dataset BEIR CQADupStack programmers...")
dataset = ir_datasets.load("beir/cqadupstack/programmers")

# Extraer documentos del dataset
docs = []
doc_ids = []

print("ğŸ”„ Extrayendo documentos...")
for doc in dataset.docs_iter():
    combined_text = ""
    if hasattr(doc, 'title') and doc.title:
        combined_text += doc.title + " "
    if hasattr(doc, 'text') and doc.text:
        combined_text += doc.text
    
    docs.append(combined_text.strip())
    doc_ids.append(doc.doc_id)

print(f"âœ… Dataset cargado: {len(docs)} documentos.")

# Aplicar preprocesamiento a los documentos
print("ğŸ”„ Aplicando preprocesamiento a los documentos...")
processed_docs = [preprocess_text(doc) for doc in docs]

# VectorizaciÃ³n con preprocesamiento
print("ğŸ”„ Vectorizando documentos...")
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(processed_docs)
print("âœ… VectorizaciÃ³n completada.")

# Mostrar estadÃ­sticas del dataset
print(f"\nğŸ“Š ESTADÃSTICAS DEL DATASET:")
print(f"   Total de documentos: {len(docs)}")
print(f"   Vocabulario TF-IDF: {len(vectorizer.vocabulary_)} tÃ©rminos")
print(f"   Matriz TF-IDF: {tfidf_matrix.shape}")

# Ejecutar la interfaz
search_interface_no_preprocess(vectorizer, tfidf_matrix, docs, doc_ids)
